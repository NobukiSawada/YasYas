thub/
│   └── workflows/
│       └── update_data.yml  # 日次データ取得のためのGitHub Action定義ファイル
├── data/
│   └── prices.json          #
フロントエンドが利用する、統合・整形済みのデータファイル
├── scripts/
│   └── data_fetcher.py      # WAGRI APIを呼び出すPythonスクリプト
│   └── requirements.txt     # Pythonの依存ライブラリ (requests, pandas)
├── js/
│   └── main.js              # データを取得しグラフを描画するメインJavaScript
├── css/
│   └── style.css            # ウェブサイトのカスタムスタイルシート
├── index.html               # メインとなるHTMLファイル
└── README.md                # プロジェクトのドキュメント
2.4.
分離型・静的ファーストアーキテクチャの利点このアーキテクチャは、従来のウェブアプリケーション開発のアプローチと比較して、極めて堅牢かつスケーラブルであり、運用コストを劇的に削減します。データ処理（短時間で終わるスケジュールタスク）とウェブ配信を分離することで、常時稼働のバックエンドサーバーの必要性を完全に取り除きます。データはリアルタイムで変動するのではなく、1日に1回更新されるという特性を持っています
7。したがって、ユーザーがアクセスするたびにAPIを呼び出すのは非効率であり、WAGRI
APIにも不要な負荷をかけます。提案するアーキテクチャでは、更新が必要なデータ（prices.json）を1日に1度だけ事前に計算し、静的な成果物としてリポジトリに保存します。その後、GitHub
Pagesがこの静的ファイルを配信するだけなので、応答速度は非常に速く、実質的に無制限のトラフィックを無料で処理できます
5。アプリケーションの「動的な」部分は、すべてユーザーのブラウザ側で処理されるため、サーバー側の負担はゼロになります。このパターンは、各技術の長所を最大限に引き出します。すなわち、データ処理にはPythonを、スケジューリングにはGitHub
Actionsを、そして高性能な静的ホスティングにはGitHub
Pagesを利用する、最もエレガントで効率的なソリューションです。3.
データ取得パイプライン：WAGRI
APIとの連携本セクションでは、システムの心臓部であるWAGRIプラットフォームからのデータ取得および処理プロセスの詳細を解説します。3.1.
WAGRIプラットフォームの利用開始手順WAGRI
APIは完全に公開されているわけではなく、利用には事前の登録が必要です。ステップ1:
利用申請:
WAGRIの利用申請ページ（https://wagri-subscription.db.naro.go.jp/ords/r/prd/subscription/pre-registration）にアクセスし、必要な情報を入力して利用申請を行います
7。ステップ2: 利用規約への同意:
申請プロセス中に、「WAGRIにおける農林水産省オープンデータAPI利用規約」が表示されます。内容をよく読み、同意する必要があります
7。ステップ3: APIキーの取得:
申請が承認されると、APIキー（またはそれに準ずる認証情報）が発行されます。このキーは機密情報であり、厳重に管理する必要があります。3.2.
APIエンドポイントの分析本プロジェクトでは、主に2つのカテゴリのAPIを利用します。青果物:
青果物市況情報取得API 7 が主要なデータソースです。特に重要なメソッドは
GetByDays/{TargetDate} であり、特定の日付のデータを一括で取得できます。畜産物:
WAGRIポータルには、食肉卸売市場調査（豚）データ取得API や
食肉卸売市場調査（牛）データ取得API など、食肉に関する個別のAPIが存在します
8。これらも同様に GetByDays/{TargetDate} メソッドを提供しています。コード一覧:
WAGRIは、APIドキュメントページ内で、市場 (MarketCode) と品目 (ItemCode)
のコード一覧をダウンロード可能なファイルとして提供しています
7。例えば、「東京市場」のデータのみ、あるいは「だいこん」のデータのみを取得するといった、特定のクエリを作成する際にこれらのコードは不可欠です。後述するPythonスクリプトでは、これらのコードを参照してAPIリクエストを構築します。3.3.
Pythonスクリプトの実装
(scripts/data_fetcher.py)データ取得を自動化するPythonスクリプトは、以下の処理を実行します。ライブラリのインポート:
requests, pandas, os, json
などの必要なライブラリをインポートします。APIキーの安全な取得:
スクリプト内にAPIキーを直接書き込む（ハードコーディングする）ことは絶対に避けます。代わりに、GitHub
ActionsのSecrets機能で設定された環境変数からAPIキーを読み込みます。クエリ対象の定義:
取得したい品目のItemCodeと市場のMarketCodeのリストを定義します。日付ループ:
直近365日間など、取得したい期間の日付をループ処理します。APIリクエスト:
各日付に対して、青果物と畜産物のAPIエンドポイントに個別にリクエストを送信します。認証のため、リクエストヘッダーにAPIキーを含めます。エラーハンドリング:
レスポンスのHTTPステータスコードをチェックし、成功（例: 200
OK）でなかった場合はエラーとして処理します。JSONパース:
各APIからのレスポンス（JSON形式）をパースします 9。データ変換・統合:
pandasライブラリを用いて、パースしたJSONデータをDataFrameに読み込みます。必要なカラム（日付、品目名、価格など）のみを選択し、欠損値を処理し、複数のAPIからのデータを一つのDataFrameに統合します。最終JSONの生成:
統合したDataFrameを、フロントエンドが要求する特定のJSON構造（後述の表2を参照）に変換します。ファイル書き込み:
生成されたJSON文字列を data/prices.json ファイルに書き込み、保存します。3.4.
異種データソースの統合処理このデータ取得スクリプトが担う重要な役割は、単なるデータ取得に留まりません。「青果物」と「畜産物」のデータは、異なるAPIエンドポイントから提供されます
7。これらのAPIは、レスポンスの構造やフィールド名が微妙に異なる可能性があります。この課題を解決するため、data_fetcher.pyスクリプトは一種のETL（Extract,
Transform, Load）プロセスとして機能します。Extract（抽出）:
複数のデータソース（青果物API、牛肉API、豚肉APIなど）からデータを抽出します。Transform（変換）:
抽出したデータから必要なフィールド（日付、品目名、価格）のみを選択し、フィールド名が異なる場合は統一します。そして、すべてのデータを単一の、一貫性のあるデータ構造に統合します。この種のデータラングリング（データ整形）にはpandasライブラリが最適です
13。Load（ロード）:
最終的にクリーンになったデータを、フロントエンドがすぐに利用できるdata/prices.jsonファイルにロード（保存）します。このように、Pythonスクリプトがバックエンドで複雑なデータ処理をすべて引き受けることで、フロントエンド（main.js）はAPIの複雑さを意識する必要がなくなり、シンプルで一貫したデータ構造を扱うだけで済むようになります。これにより、フロントエンドの開発は大幅に簡素化され、システム全体の保守性が向上します。表1:
WAGRI APIリクエストパラメータの例
(青果物市況情報API)この表は、開発者がAPIクエリをカスタマイズする際の参考情報となります。パラメータ型場所説明例TargetDatestringURLパスデータを取得する対象の日付。YYYY-MM-DD形式。2023-10-26AuthorizationstringヘッダーAPIサブスクリプションキー。Bearer
{YOUR_API_KEY}$filterstringクエリ文字列特定のフィールドでデータをフィルタリングするためのOData構文。$filter=MarketCode
eq
'51300'$selectstringクエリ文字列レスポンスに含めるフィールドを選択するためのOData構文。$select=Date,ItemName,Price表2:
prices.jsonの提案されるデータ構造このJSON構造は、バックエンドのデータ処理とフロントエンドの描画処理の間の重要なインターフェースを定義します。Chart.jsはX軸用のラベル配列とY軸用のデータセット配列を要求するため
23、この形式はJavaScript側での複雑なデータ操作を不要にし、効率的なグラフ描画を可能にします。JSON{
  "キャベツ": {
    "labels": ["2023-10-01", "2023-10-02",...],
    "prices": [150, 152,...]
  },
  "だいこん": {
    "labels": ["2023-10-01", "2023-10-02",...],
    "prices": [98, 99,...]
  },
  "国産牛肉": {
    "labels": ["2023-10-01", "2023-10-02",...],
    "prices": [850, 855,...]
  }
}
4. GitHub
Actionsによる自動化ワークフロー本セクションでは、データ更新からデプロイまでを全自動で行うCI/CDパイプラインの具体的な設定方法について解説します。4.1.
ワークフローのトリガー：スケジュール実行ワークフローは、POSIX
cron構文を用いたスケジュール実行によって起動されます 15。WAGRI
APIのデータ更新は日本時間15時頃（協定世界時(UTC)で06:00）に行われます
7。データが確実に更新されていること、また毎時0分などの高負荷時間帯を避けるため
1、cronジョブは**30 7 * * ***（毎日07:30
UTC）に設定するのが賢明です。加えて、GitHubのUIから手動でワークフローを実行できるよう、workflow_dispatchトリガーも設定します
25。4.2. 認証情報の安全な管理WAGRI
APIキーは機密情報であり、リポジトリに直接コミットしてはいけません。APIキーは、GitHubリポジトリの「Settings」→「Secrets
and
variables」→「Actions」にて、リポジトリシークレットとして保存します（例：WAGRI_API_KEY）。ワークフローファイル内では、${{
secrets.WAGRI_API_KEY
}}という構文を用いてこのシークレットに安全にアクセスします。4.3.
ワークフローのジョブステップ
(.github/workflows/update_data.yml)YAMLファイルに定義するワークフローは、以下のステップで構成されます。コードのチェックアウト:
actions/checkout@v3アクションを使用し、リポジトリのソースコードをランナー（実行環境）に展開します。Pythonのセットアップ:
actions/setup-python@v4アクションを使用し、指定したバージンのPython環境を準備します。依存関係のインストール:
pip install -r
scripts/requirements.txtコマンドを実行し、requestsやpandasなどの必要なライブラリをインストールします。データ取得スクリプトの実行:
python
scripts/data_fetcher.pyコマンドを実行します。この際、リポジトリシークレットに保存したWAGRI
APIキーを環境変数としてスクリプトに渡します。変更のコミットとプッシュ:
gitコマンドを用いて、data/prices.jsonファイルに変更があったかどうかを確認します。変更があった場合のみ、新しいファイルをコミットし、mainブランチにプッシュしてリポジトリを更新します。4.4.
自動コミットと冪等性の確保このワークフローの高度な側面は、ワークフロー自身がリポジトリに変更をコミットする点と、その処理を冪等（べきとう）に保つ点にあります。ワークフローからリポジトリに書き込み（プッシュ）を行うには、特別な権限が必要です。デフォルトでワークフローに提供されるGITHUB_TOKENは、セキュリティ上の理由から読み取り権限しか持っていません。書き込みを行うためには、repoおよびworkflowスコープを持つ**パーソナルアクセストークン（PAT）**を別途生成し、それをGH_PATのような名前でリポジトリシークレットに保存する必要があります 27。また、毎日ワークフローを実行すると、APIから取得したデータに変化がない日（例えば祝日など）でも、空のコミットが作られてしまい、Gitの履歴が不要な情報で溢れてしまいます。これを避けるため、コミットを実行する前にデータの変更を検知するステップを設けることが重要です。git diff --exit-codeのようなコマンドを使い、ファイルに変更があった場合のみ後続のコミット・プッシュ処理を実行するように制御します。このように、セキュリティ（PATの利用）と効率性（変更があった場合のみコミット）を考慮することで、プロフェッショナルレベルの堅牢な自動化ワークフローが完成します。5. フロントエンド実装：Chart.jsによるデータ可視化本セクションでは、ユーザーが直接目にするウェブサイトの構築について説明します。5.1. HTML構造 (index.html)ウェブページの骨格となるHTMLファイルは、以下の要素で構成されます。基本的なHTML5の定型文。レスポンシブデザインのための<meta name="viewport">タグ。カスタムスタイルシートstyle.cssへのリンク。ページ全体のコンテンツをまとめるメインコンテナ<div>。サイトのタイトルを表示する<h1>タグ。表示したいグラフごとに、一意のidを持つ専用の<canvas>要素（例: <canvas id="vegetable-chart"></canvas>）。Chart.jsライブラリをCDN経由で読み込む<script>タグ。CDNを利用することで、自身でライブラリをホストする手間が省けます 3。<body>タグの末尾に、defer属性を付けてカスタムJavaScriptファイルmain.jsを読み込む<script>タグ。5.2. JavaScriptロジック (js/main.js)グラフの描画を制御するJavaScriptは、非同期処理を適切に扱う必要があります。ステップ1: データ取得: fetch() APIを用いて、ローカルに配置されたdata/prices.jsonファイルを非同期で読み込みます。これはサーバーへのリクエストではなく、同一オリジン内のファイルへのアクセスです 16。ステップ2: データ処理: 読み込みが完了したら、JSONデータをパースしてJavaScriptオブジェクトに変換します。ステップ3: グラフ描画: JSONオブジェクト内の各品目（例: "キャベツ", "国産牛肉"）に対して、以下の処理を繰り返します。対応する<canvas>要素の描画コンテキストを取得します: const ctx = document.getElementById('vegetable-chart').getContext('2d'); 3。グラフの設定を定義するコンフィグオブジェクトを作成します。type: 'line' を指定して折れ線グラフを選択します 23。dataプロパティ内で、JSONから取得したlabels配列（日付）とprices配列（価格）を、それぞれグラフのlabelsとdatasetsに割り当てます。データセットには、凡例に表示されるlabel（品目名）や、線の色(borderColor)、曲線の張り(tension)などのスタイルオプションも設定できます 23。new Chart(ctx, config); を呼び出し、グラフをインスタンス化して描画します 24。5.3. スタイルとユーザーエクスペリエンス (css/style.css)ページのレイアウト、タイポグラフィ、グラフのコンテナなどに対して基本的なスタイルを適用し、クリーンで可読性の高い表示を確保します。Chart.jsは親コンテナのサイズに基づいて自動的にリサイズするため、コンテナの幅をパーセンテージで指定するなどして、レスポンシブデザインに対応させます。5.4. 非同期処理とユーザーフィードバックfetch()によるデータ取得は非同期で行われるため、ウェブページが表示されてからグラフが描画されるまでに、わずかな時間差が生じる可能性があります。特にデータファイルが大きい場合、この遅延はユーザーに知覚されるかもしれません。この時間差の間、ユーザーにはグラフが表示されるべき場所に空白が見えてしまいます。より良いユーザーエクスペリエンスを提供するためには、データ読み込み中に何らかのフィードバックを示すことが推奨されます。具体的な実装としては、HTMLにあらかじめ「読み込み中...」というテキストやスピナー（回転するアイコン）を配置しておきます。そして、JavaScript側でfetch()が完了し、グラフの描画が成功した後に、このローディングインジケーターを非表示にします。これにより、ユーザーはアプリケーションが正常に動作していることを即座に認識でき、待ち時間のストレスが軽減されます。このような細やかな配慮が、プロフェッショナルなフロントエンド実装と、そうでないものとを分ける重要な要素となります。6. 総合開発・デプロイロードマップ本セクションでは、プロジェクトの開始から公開、そして運用に至るまでの全工程を、具体的なステップバイステップのチェックリストとして提示します。ステップ1: GitHubの初期設定GitHub上で新しい**公開 (Public)**リポジトリを作成します。作成したリポジトリをローカルマシンにクローンします。セクション2.3で定義したディレクトリ構造（.github/workflows, data, scripts, js, css）をローカルで作成します。GitHubリポジトリの「Settings」タブ → 「Pages」セクションに移動し、Sourceとしてmainブランチを選択して保存します。これによりGitHub Pagesが有効化され、サイトのURLが生成されます 2。ステップ2: WAGRI APIの利用準備WAGRIプラットフォームに登録し、APIキーを取得します 7。GitHubリポジトリの「Settings」→「Secrets and variables」→「Actions」で、WAGRI_API_KEYという名前の新しいリポジトリシークレットを作成し、取得したAPIキーを値として設定します。GitHubの「Developer settings」から、repoとworkflowのスコープを持つパーソナルアクセストークン（PAT）を生成します。これをGH_PATという名前で同様にリポジトリシークレットとして保存します。ステップ3: バックエンドスクリプトの開発ローカル環境でscripts/data_fetcher.pyスクリプトを開発します。ローカルでのテスト中は、APIキーを.envファイルに記述して管理すると便利です（ただし、.envファイルは必ず.gitignoreに追加し、リポジトリにコミットしないように注意してください）。スクリプトをローカルで実行し、data/prices.jsonファイルが正しい形式で生成されることを確認します。ステップ4: フロントエンドの開発index.html, style.css, js/main.jsの各ファイルを作成します。ステップ3で生成したdata/prices.jsonをモックデータ（仮のデータ）として使用します。このローカルJSONファイルを読み込み、Chart.jsを用いてグラフを描画するJavaScriptコードを開発します。VS Codeの「Live Server」拡張機能などを用いてローカルサーバーを起動し、ブラウザで表示を確認しながら開発を進めます。ステップ5: GitHub Actionsワークフローの作成.github/workflows/ディレクトリ内にupdate_data.ymlファイルを作成します。コードのチェックアウト、Pythonのセットアップ、依存関係のインストール、スクリプトの実行（シークレットを利用）、結果のコミット（PATを利用）といった一連のステップを含む、完全なワークフローを記述します。ステップ6: デプロイと検証ローカルで作成したすべてのファイル（HTML, スクリプト, ワークフローファイルなど）をコミットし、GitHubのmainブランチにプッシュします。プッシュをトリガーとしてワークフローが自動的に実行されます。GitHubリポジトリの「Actions」タブで、その実行状況を監視します。ワークフローの実行でエラーが発生した場合は、ログを確認してデバッグします。アクションが正常に完了すると、ワークフローによってdata/prices.jsonが追加・更新された新しいコミットが作成されます。自身のGitHub PagesのURL（https://<username>.github.io/<repository-name>/）にアクセスし、ウェブサイトが公開され、グラフが正しく表示されていることを確認します。ステップ7: 継続的なメンテナンスGitHub Actionsの実行結果を定期的に監視し、エラーが発生していないかを確認します。WAGRI APIの仕様は将来変更される可能性があります（例として、ドキュメントには2025年に向けた新URLが記載されています 7）。APIの仕様変更があった場合は、data_fetcher.pyスクリプトを更新する必要があります。Chart.jsなどのフロントエンドライブラリは、定期的に最新の安定バージョンに更新することが推奨されます。
